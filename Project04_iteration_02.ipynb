{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data downloaded from https://www.kaggle.com/extralime/math-lectures/version/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:03.489171Z",
     "start_time": "2019-11-13T21:52:57.168286Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:03.573881Z",
     "start_time": "2019-11-13T21:53:03.569127Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:32.388031Z",
     "start_time": "2019-11-13T21:53:03.649407Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# takes ~30 seconds to import\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:32.525071Z",
     "start_time": "2019-11-13T21:53:32.519524Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:32.619979Z",
     "start_time": "2019-11-13T21:53:32.611908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Reduce Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:33.131446Z",
     "start_time": "2019-11-13T21:53:32.798965Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:34.148215Z",
     "start_time": "2019-11-13T21:53:33.226029Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:34.234009Z",
     "start_time": "2019-11-13T21:53:34.229936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:34.351128Z",
     "start_time": "2019-11-13T21:53:34.346573Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:47.131343Z",
     "start_time": "2019-11-13T21:53:34.450052Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/metis/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import seaborn as sns\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:47.238711Z",
     "start_time": "2019-11-13T21:53:47.233882Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:48.415609Z",
     "start_time": "2019-11-13T21:53:47.447078Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"raw_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:48.959050Z",
     "start_time": "2019-11-13T21:53:48.754387Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>In this sequence of segments,\\nwe review some ...</td>\n",
       "      <td>Probability</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>CS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        label  le\n",
       "0  The following content is\\nprovided under a Cre...     Calculus   3\n",
       "1  In this sequence of segments,\\nwe review some ...  Probability   9\n",
       "2  The following content is\\nprovided under a Cre...           CS   2\n",
       "3  The following\\ncontent is provided under a Cre...   Algorithms   1\n",
       "4  The following\\ncontent is provided under a Cre...   Algorithms   1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['label'])\n",
    "\n",
    "df['le'] = le.transform(df['label']) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:49.227694Z",
     "start_time": "2019-11-13T21:53:49.126514Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>le</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Linear Algebra</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Probability</td>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CS</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Diff. Eq.</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Algorithms</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Statistics</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Calculus</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Data Structures</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AI</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Math for Eng.</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NLP</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text   le\n",
       "label                     \n",
       "Linear Algebra    152  152\n",
       "Probability       124  124\n",
       "CS                104  104\n",
       "Diff. Eq.          93   93\n",
       "Algorithms         81   81\n",
       "Statistics         79   79\n",
       "Calculus           70   70\n",
       "Data Structures    62   62\n",
       "AI                 48   48\n",
       "Math for Eng.      28   28\n",
       "NLP                19   19"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_sorted = df.groupby('label').count().sort_values(['text'], ascending=False)\n",
    "grouped_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:49.423850Z",
     "start_time": "2019-11-13T21:53:49.415300Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3\n",
       "1      9\n",
       "2      2\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "855    7\n",
       "856    5\n",
       "857    6\n",
       "858    0\n",
       "859    5\n",
       "Name: le, Length: 860, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['le']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:49.630161Z",
     "start_time": "2019-11-13T21:53:49.626025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df.iloc[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### investigate stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:50.045015Z",
     "start_time": "2019-11-13T21:53:49.822884Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION 1: Standardize  \n",
    "FUNCTION 2: Vectorize  \n",
    "FUNCTION 3: Reduce Dimensions  \n",
    "FUNCTION 4: Cluster/Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 1: Standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas: MWE, SpaCy (lemmatization), tokenization.  \n",
    "Create list of custom stop words to remove from raw data.  \n",
    "- going, just, let, minus, professor, audience, plus, okay, print, mit, respect, gonna, just, \n",
    "\n",
    "Create Compound Terms. (from nltk.tokenize import MWETokenizer # multi-word expression)  \n",
    "- Laplace Transform\n",
    "- Partial Derivative\n",
    "- (variable) squared\n",
    "- Power Series Expansion\n",
    "\n",
    "Stem.  \n",
    "- Probability vs. Probabilities\n",
    "- Matrix vs. Matrices\n",
    "- event vs. events\n",
    "- edge vs. edges\n",
    "- transform vs. transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:53:50.261655Z",
     "start_time": "2019-11-13T21:53:50.257469Z"
    }
   },
   "outputs": [],
   "source": [
    "# Figure out the MWETokenizer function; split them up and then join them back together.\n",
    "# my_text = \"You all are the greatest students of all time.\"\n",
    "# mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "# mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "\n",
    "# mwe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:54:00.436010Z",
     "start_time": "2019-11-13T21:53:50.499649Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "tokenizer = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:54:00.810691Z",
     "start_time": "2019-11-13T21:54:00.642028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word probability word probabilitie'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_lemma(text):\n",
    "        \n",
    "    text_obj = tokenizer(text, disable=['parser', 'ner'])\n",
    "    \n",
    "    text_lemma = ' '.join([token.lemma_ for token in text_obj if not token.is_stop])\n",
    "  \n",
    "    return text_lemma\n",
    "\n",
    "test_text = \"The word probability is the same as the word probabilities\"\n",
    "tokenize_lemma(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:54:01.059573Z",
     "start_time": "2019-11-13T21:54:01.035880Z"
    }
   },
   "outputs": [],
   "source": [
    "def standardize(df):\n",
    "    # replace '\\n' with whitespace\n",
    "    newline = lambda x: re.sub('\\n', ' ', x)\n",
    "    \n",
    "    # remove numbers\n",
    "    alphanumeric = lambda x: re.sub('\\w*\\d\\w*|&gt;', '', x)\n",
    "    # remove punctuation, convert to lowercase\n",
    "    punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x.lower())\n",
    "    \n",
    "    lemmatize = lambda x: tokenize_lemma(x)\n",
    "    \n",
    "    # replace all \"taylor's\" with \"taylor\"\n",
    "    df['text'] = df['text'].str.replace(\"taylor's\", \"taylor\", case = False)\n",
    "    df['text'] = df['text'].str.replace(\"transformation\", \"transform\", case = False)\n",
    "    \n",
    "    # replace all of (custom stop words list) with empty string\n",
    "    custom_stop = ['going','just','let','minus','professor','audience','play','okay','print','mit','respect','gonna','ok','thats','im','right','tes']\n",
    "    rem_stop = lambda x: re.sub(\"|\".join(custom_stop), \"\", x)\n",
    "    \n",
    "    df['text'] = df.text.map(newline).map(alphanumeric).map(punc_lower)\n",
    "    df['text'] = df['text'].str.replace(\"laplace transform\", \"laplace_transform\", case = False)\n",
    "    df['text'] = df['text'].str.replace(\"partial derivative\", \"partial_derivative\", case = False)\n",
    "    df['text'] = df['text'].str.replace(\"power series expansion\", \"power_series_expansion\", case = False)\n",
    "    df['text'] = df.text.map(rem_stop)\n",
    "    df['text'] = df.text.map(lemmatize)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:54:12.648228Z",
     "start_time": "2019-11-13T21:54:12.621570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>In this sequence of segments,\\nwe review some ...</td>\n",
       "      <td>Probability</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>CS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>The following\\ncontent is provided under a Cre...</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Math for Eng.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>&amp;gt;&amp;gt; [MUSIC] &amp;gt;&amp;gt; DAVID J. MALAN: All ...</td>\n",
       "      <td>Diff. Eq.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>857</td>\n",
       "      <td>The following content is\\nprovided by MIT Open...</td>\n",
       "      <td>Linear Algebra</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858</td>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>AI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859</td>\n",
       "      <td>The following content is\\nprovided under a Cre...</td>\n",
       "      <td>Diff. Eq.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>860 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text           label  le\n",
       "0    The following content is\\nprovided under a Cre...        Calculus   3\n",
       "1    In this sequence of segments,\\nwe review some ...     Probability   9\n",
       "2    The following content is\\nprovided under a Cre...              CS   2\n",
       "3    The following\\ncontent is provided under a Cre...      Algorithms   1\n",
       "4    The following\\ncontent is provided under a Cre...      Algorithms   1\n",
       "..                                                 ...             ...  ..\n",
       "855  The following content is\\nprovided under a Cre...   Math for Eng.   7\n",
       "856  &gt;&gt; [MUSIC] &gt;&gt; DAVID J. MALAN: All ...       Diff. Eq.   5\n",
       "857  The following content is\\nprovided by MIT Open...  Linear Algebra   6\n",
       "858  The following content is\\nprovided under a Cre...              AI   0\n",
       "859  The following content is\\nprovided under a Cre...       Diff. Eq.   5\n",
       "\n",
       "[860 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:59:01.805543Z",
     "start_time": "2019-11-13T21:54:22.968724Z"
    }
   },
   "outputs": [],
   "source": [
    "df_s = standardize(df) # executes in about 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:13:49.105911Z",
     "start_time": "2019-11-13T22:13:49.070712Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'follow content provide creative common license support help   opencourseware continue offer high quality educational resource free donation view additional material hundred   course visit   opencourseware ocwedu     jerison relax sunny london ontario today send substitute   glad agenda today say would talk power series taylor formula guess week   friday    little example application course evaluation survey ill hand   minu class handout say   end term   not pick come grab   people tend pick walk grab   s thing miss decide office hour end term   not decide check website information loe forward final exam uh   not question technical stuff   s talk power series little bit think review story power series   attention power series way write function sum integral power x    number example power series polynomial forget type power series go finite number term end high ais   perfectly good example power series special kind power series want tell today power series behave exactly like polynomial s   thing careful power series not concern polynomial ill minute think generalize polynomial thing careful number caution s number ill r r   infinity number   infinity inclusive absolute value x r x small r size sum converge sum sum converge finite value x big r absolute value sum diverge r call radius convergence example radius convergence power series find   property power series think   jerison talk earlier s radius convergence here inside radius convergence function derivative derivative   like polynomial differentiate term derivative number power series express term value derivative   call taylor formula   say inside radius convergence function loe fx write value function     plus value derivative bracket n mean derivative n tes n   derivative   divide     multiply x   linear term power series quadratic term second derivative remember divide     multiply x term coefficient power series   record value derivative function x    compute way s think   end summary thing talk think example ill repeat example power series example not david jerison leonard euler example function exponential function ex s s compute   repeat computation power series ex   portant thing order know derivative ex second derivative ex come taylor formula coefficient know derivative ex   ex way way derivative ex evaluate x    value ex   value ex   x    value   way derivative   value   plug formula find ex   plus x plus   x plus   x plus number   wind factorial denominator   power series ex discovery leonhard euler   yes maam   write power series far write   far write power series define satisfactory solution exam problem suppose way phrase question pattern pattern s doubt term people tell write summation convention thing not believe   term clear   good   answer   yes thank      basic example s basic example power series oh yes way write power series radius convergence   tell radius convergence power series infinity sum converge value x ill little minu yeah   function write power series   function write power series   excellent question function reasonable expression write power series   give good answer true answer little bit complicated function occur calculus like sin cosine tangent power_series_expansion   example s example here example guess example example think newton euler s find power_series_expansion function x think line learn geometric series tell tell answer ill   write geometric series tell function write alternate sum power x wonder es come think geometric series probably remember   sign get replace   sign think maybe jerison talk here basic example remember graph function los like x    s little problem denominator   graph pole go infinity x      indication radius convergence infinity try converge infinite number put x    will big problem fact x    get   term get big big converge example radius convergence    s new example oh way calculate number taylor formula not see check calculate iterate derivative function plug x         yes sir   radius convergence   will blow   like fine   question s problem x    problem x    graph perfectly smooth innocuous finite   excellent question problem radius   direction s problem     radius convergence happen x    s lo partial sum x    mind ill partial sum           not infinity converge     thing fail converge example   real number edge    s different example trig function sine x    compute power_series_expansion sinx    taylor formula taylor formula say start compute derivative sinx sound like   lot work s derivative sine cosine derivative cosine   second derivative sine remember   sinx   want derivative sine derivative sine pre pre derivative   decide derivative sine cosine cosine   sign want differentiate cosine   sine sign cancel   sign sinx follow lot s cancel sudden    start pattern repeat forever high high derivative sine   plus   sine cosine taylor formula say substitute x    happen s x equal   sine   cosine   sine        cosine   s     start pattern repeat   value derivative zero plus   one pattern fourfold periodicity write sinx taylor formula formula value     derivative   multiply x second derivative divide   second derivative        drop term derivative   remember   denominator   coefficient x s fourth derivative board   drop term fifth term fifth power x derivative   have go pattern   value iterate derivative   x tell term pattern guess term   x   write   x    x   x    plus x    guess   power_series_expansion sine x   sign alternate denominator big not exponential grow fast   remark r infinity radius convergence power series infinity     reason general term   like x    odd number write     want size happen size n go infinity s   think fix x s fix number x lo power x think size expression n get large s   second x    write like x    tes x     sorry   tes x    tes x    have multiply x   te numerator have multiply number      denominator give factorial have   write like x fix maybe million   big fix happen number pretty big         n get maybe n     n billion    denominator get big big numerator stay x product far    multiply small number matter x number converge   will small small x get big   sign x inside radius convergence sign series converge value x x work convergence   fix x   tell radius convergence infinity formula fact property radius convergence talk r equal infinity condition x number infinity absolute value convergence   general term work x radius convergence infinity kind fast think have hear earlier have get sine function new function power series way compute sinx term will good evaluation sinx x tell lot function sinx example formula hard sine x periodic obvious hide away expression number pi half period   clear power series power series good thing hide property function want spend minu tell power series new power series new power series old call operation power series thing power series thing multiply example want compute power series x sinx power series sinx   power series x actually function x sple polynomial polynomial       coefficient   x power series sple sinx power series want encourage treat power series   like polynomial multiply operation compute power series x sinx   multiply x s   distribu x   x    plus x    radius convergence   small radii convergence r equal infinity case   multiply power series pain power series long x pretty sple    thing notice way know odd function sine odd function x odd function product odd function function   reflect fact power occur power series odd function like sine power occur odd power x   true   multiply differentiate s   case use process differentiation find power series cosx write cosx derivative sine differentiate term term ill expression power series sine differentiate term term ill power series cosine s derivative x derivative x   s   denominator derivative x   s   denominator cancellation happen      cancel factor   factorial leave     cancel factor   factorial leave   denominator s power_series_expansion cosine get power x alternate factorial denominator course derive expression taylor formula kind calculation take high high derivative cosine periodic pattern derivative value derivative x    here clean way spler way know derivative sine differentiate radius convergence   multiply add multiply constant thing like integrate   half course not s integrate integration    integral   x dt   x integral function find antiderivative lnt evaluate t   x lnx evaluate natural log   ln       valid way x big   not want think like x small    try apply power series method find use integral find power series natural log ill plug expression power series t know write board change variable x t t    t plus t   t   thing inside integral legal integrate term term s    evaluate x   integrate   x integrate t t   sorry integrate t t    t give t    t   x   replace ts xs t      equal x have discover lnx x   x    plus x     x    s power_series_expansion lnx begin power series radius convergence    begin power series radius convergence     function   point function go bad x   problem happen   reflect radius convergence cool integrate correct power_series_expansion lnx victory euler use kind power_series_expansion calculate natural logarithm efficient way people   property think    substitute appropriate substitute teacher tell substitution    try find power_series_expansion et   way ill take power_series_expansion ex substitution x   t expansion ex question     concern radius convergence not define x positive not radius convergence    like worry lnx function perfectly behave large x power series fail converge large x suppose x big   big big power x grow infinity grow large faster number       grow exponentially   grow linearly general term x big general term infinity function talk log net   plus x perfectly good power series good outside radius convergence   fact life yes   inaudible   would talk class question small radii convergence basic answer not expect big small power series give information inside range function   inaudible   case radii convergence infinity x radius convergence infinity sure sinx infinity case    s      integrate will end te today s power_series_expansion power_series_expansion   function t   variable t take expansion ex put x term t whoop   t place x series expansion ex work little bit better t   t   squared   plus t    t will   sign t denominator   sign   alternate power denominator factorial te course go error function appearance error function guess get normalize put   square root pi integral et dt   x normalization x get large value   error function portant theory probability think calculate fact point course standard definition error function   square root pi s calculate power_series_expansion s   square root pi hurt want integrate et    use power_series_expansion     write think carefully example ill little quicker integrate term term   integrate power t pretty sple   evaluate x   x   x    plus x     integrate t   denominator s x    plus agine guess exactly form begin talk multiply coefficient x   square root pi coefficient x     te square root pi perfectly good way write power_series_expansion good way compute value error function new function experience calculator probably calcula calculator probably method     sermon example thing power series   ceg thing   minute   jerison want ad    case think take term lot thing course context thing vector calculus will learn vector thing like come explain thing course little bit strange like strange formula product rule quotient rule sort random formula thing learn   special case chain rule   drive point home want poem drive point home forcefully think'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T22:28:27.165506Z",
     "start_time": "2019-11-12T22:28:27.156350Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_s.iloc[42][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2: Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:14:24.214754Z",
     "start_time": "2019-11-13T22:14:24.198460Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_tfidf(df):\n",
    "    '''\n",
    "    Creates a sparse matrix of counts with Tf-Idf Vectorizer\n",
    "    '''\n",
    "    # define series to go into vectorizer\n",
    "    x = df['text']\n",
    "    # define vectorizer\n",
    "    cv_tfidf = TfidfVectorizer(min_df=2, max_df=0.5)\n",
    "    # vectorize: convert to sparse matrix\n",
    "    sparse_matrix = cv_tfidf.fit_transform(x)\n",
    "    feature_names = cv_tfidf.get_feature_names()\n",
    "    # print the matrix\n",
    "    print(pd.DataFrame(sparse_matrix.toarray(), columns=feature_names))\n",
    "    # return the sparse matrix and feature names\n",
    "    return sparse_matrix, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:14:28.518547Z",
     "start_time": "2019-11-13T22:14:26.540742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      aa  aab  aah  aardvark  aaron        ab   abandon  abbreviate  \\\n",
      "0    0.0  0.0  0.0       0.0    0.0  0.000000  0.000000         0.0   \n",
      "1    0.0  0.0  0.0       0.0    0.0  0.000000  0.000000         0.0   \n",
      "2    0.0  0.0  0.0       0.0    0.0  0.000000  0.000000         0.0   \n",
      "3    0.0  0.0  0.0       0.0    0.0  0.000000  0.000000         0.0   \n",
      "4    0.0  0.0  0.0       0.0    0.0  0.000000  0.000000         0.0   \n",
      "..   ...  ...  ...       ...    ...       ...       ...         ...   \n",
      "855  0.0  0.0  0.0       0.0    0.0  0.000000  0.000000         0.0   \n",
      "856  0.0  0.0  0.0       0.0    0.0  0.000000  0.000000         0.0   \n",
      "857  0.0  0.0  0.0       0.0    0.0  0.000000  0.000000         0.0   \n",
      "858  0.0  0.0  0.0       0.0    0.0  0.000000  0.000000         0.0   \n",
      "859  0.0  0.0  0.0       0.0    0.0  0.109389  0.021198         0.0   \n",
      "\n",
      "     abbreviation  abc  ...  zoo      zoom   zp  zpk   zr        zs   zt   zx  \\\n",
      "0             0.0  0.0  ...  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
      "1             0.0  0.0  ...  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
      "2             0.0  0.0  ...  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
      "3             0.0  0.0  ...  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
      "4             0.0  0.0  ...  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
      "..            ...  ...  ...  ...       ...  ...  ...  ...       ...  ...  ...   \n",
      "855           0.0  0.0  ...  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
      "856           0.0  0.0  ...  0.0  0.007931  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
      "857           0.0  0.0  ...  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
      "858           0.0  0.0  ...  0.0  0.000000  0.0  0.0  0.0  0.000000  0.0  0.0   \n",
      "859           0.0  0.0  ...  0.0  0.000000  0.0  0.0  0.0  0.017503  0.0  0.0   \n",
      "\n",
      "      zy   zz  \n",
      "0    0.0  0.0  \n",
      "1    0.0  0.0  \n",
      "2    0.0  0.0  \n",
      "3    0.0  0.0  \n",
      "4    0.0  0.0  \n",
      "..   ...  ...  \n",
      "855  0.0  0.0  \n",
      "856  0.0  0.0  \n",
      "857  0.0  0.0  \n",
      "858  0.0  0.0  \n",
      "859  0.0  0.0  \n",
      "\n",
      "[860 rows x 10502 columns]\n"
     ]
    }
   ],
   "source": [
    "sm_v, feature_names = vectorize_tfidf(df_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:14:42.749837Z",
     "start_time": "2019-11-13T22:14:42.742681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(860, 10502)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:14:44.123845Z",
     "start_time": "2019-11-13T22:14:44.092000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aab',\n",
       " 'aah',\n",
       " 'aardvark',\n",
       " 'aaron',\n",
       " 'ab',\n",
       " 'abandon',\n",
       " 'abbreviate',\n",
       " 'abbreviation',\n",
       " 'abc',\n",
       " 'abcd',\n",
       " 'ability',\n",
       " 'ablate',\n",
       " 'ablation',\n",
       " 'abnormal',\n",
       " 'abort',\n",
       " 'abraham',\n",
       " 'abrupt',\n",
       " 'abs',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absorb',\n",
       " 'abstract',\n",
       " 'abstraction',\n",
       " 'abstractly',\n",
       " 'absurd',\n",
       " 'abumostafa',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abuse',\n",
       " 'abx',\n",
       " 'ac',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'accelerate',\n",
       " 'acceleration',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accommodate',\n",
       " 'accomplish',\n",
       " 'accord',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accountable',\n",
       " 'accountably',\n",
       " 'accounting',\n",
       " 'accumula',\n",
       " 'accumulate',\n",
       " 'accumulated',\n",
       " 'accumulative',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accustomed',\n",
       " 'ace',\n",
       " 'ache',\n",
       " 'achievable',\n",
       " 'achieve',\n",
       " 'achievement',\n",
       " 'acid',\n",
       " 'ackermann',\n",
       " 'acknowledge',\n",
       " 'acknowledgement',\n",
       " 'acl',\n",
       " 'acoustic',\n",
       " 'acquaint',\n",
       " 'acquire',\n",
       " 'acronym',\n",
       " 'act',\n",
       " 'actg',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'activate',\n",
       " 'activation',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'actuality',\n",
       " 'actuary',\n",
       " 'acute',\n",
       " 'acyclic',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'adamsbashforth',\n",
       " 'adapt',\n",
       " 'adaptive',\n",
       " 'adaptively',\n",
       " 'adbc',\n",
       " 'adder',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additive',\n",
       " 'additivity',\n",
       " 'addon',\n",
       " 'address',\n",
       " 'addressee',\n",
       " 'addressing',\n",
       " 'ade',\n",
       " 'adequate',\n",
       " 'adhere',\n",
       " 'adiabatic',\n",
       " 'adjacency',\n",
       " 'adjacent',\n",
       " 'adjectival',\n",
       " 'adjective',\n",
       " 'adjoint',\n",
       " 'adleman',\n",
       " 'adment',\n",
       " 'administer',\n",
       " 'administration',\n",
       " 'administrative',\n",
       " 'administrator',\n",
       " 'administrivia',\n",
       " 'admire',\n",
       " 'admissibility',\n",
       " 'admissible',\n",
       " 'admission',\n",
       " 'ado',\n",
       " 'adopt',\n",
       " 'adt',\n",
       " 'adte',\n",
       " 'adtedly',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advantage',\n",
       " 'advantageous',\n",
       " 'advection',\n",
       " 'advent',\n",
       " 'adventure',\n",
       " 'adventurous',\n",
       " 'adverb',\n",
       " 'adversarial',\n",
       " 'adversary',\n",
       " 'adverse',\n",
       " 'advertise',\n",
       " 'advertisement',\n",
       " 'advice',\n",
       " 'advisable',\n",
       " 'advise',\n",
       " 'advisee',\n",
       " 'adviser',\n",
       " 'advisor',\n",
       " 'advocate',\n",
       " 'ae',\n",
       " 'aed',\n",
       " 'aerial',\n",
       " 'aesthetic',\n",
       " 'aesthetically',\n",
       " 'afar',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affine',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'afternoon',\n",
       " 'afterward',\n",
       " 'ag',\n",
       " 'agaric',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agenet',\n",
       " 'agent',\n",
       " 'agglomerate',\n",
       " 'aggrega',\n",
       " 'aggregate',\n",
       " 'aggregation',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'aginary',\n",
       " 'agination',\n",
       " 'aginative',\n",
       " 'agine',\n",
       " 'agining',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreement',\n",
       " 'agreementdisagreement',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahha',\n",
       " 'ai',\n",
       " 'aibj',\n",
       " 'aid',\n",
       " 'aij',\n",
       " 'aik',\n",
       " 'ain',\n",
       " 'ainverse',\n",
       " 'air',\n",
       " 'aircraft',\n",
       " 'airline',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'ais',\n",
       " 'aj',\n",
       " 'ajax',\n",
       " 'aji',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'akamai',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alas',\n",
       " 'albeit',\n",
       " 'albuquerque',\n",
       " 'alert',\n",
       " 'alex',\n",
       " 'alexa',\n",
       " 'alexander',\n",
       " 'algebra',\n",
       " 'algebraic',\n",
       " 'algebraically',\n",
       " 'algo',\n",
       " 'algorithm',\n",
       " 'algorithmic',\n",
       " 'algorithmically',\n",
       " 'algorithms',\n",
       " 'alia',\n",
       " 'alias',\n",
       " 'aliase',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'alien',\n",
       " 'align',\n",
       " 'alignment',\n",
       " 'alike',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allege',\n",
       " 'allen',\n",
       " 'alley',\n",
       " 'alloca',\n",
       " 'allocate',\n",
       " 'allocating',\n",
       " 'allocation',\n",
       " 'allow',\n",
       " 'allowable',\n",
       " 'allowance',\n",
       " 'allpairs',\n",
       " 'allportant',\n",
       " 'allude',\n",
       " 'ally',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'alp',\n",
       " 'alpha',\n",
       " 'alphabet',\n",
       " 'alphabeta',\n",
       " 'alphabetical',\n",
       " 'alphabetically',\n",
       " 'alphai',\n",
       " 'alphan',\n",
       " 'alphanumeric',\n",
       " 'alphas',\n",
       " 'alt',\n",
       " 'alter',\n",
       " 'alterna',\n",
       " 'alternate',\n",
       " 'alternation',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'altitude',\n",
       " 'altogether',\n",
       " 'alu',\n",
       " 'alumni',\n",
       " 'alumnus',\n",
       " 'amartya',\n",
       " 'amateur',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazement',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'ambient',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambitious',\n",
       " 'amd',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'amiss',\n",
       " 'amn',\n",
       " 'amortization',\n",
       " 'amortize',\n",
       " 'amortized',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'ampersand',\n",
       " 'ampgt',\n",
       " 'amplify',\n",
       " 'amplitude',\n",
       " 'amplt',\n",
       " 'ampltampgt',\n",
       " 'amt',\n",
       " 'amuse',\n",
       " 'amusement',\n",
       " 'amusing',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'anachronistic',\n",
       " 'anaconda',\n",
       " 'anal',\n",
       " 'analagous',\n",
       " 'analog',\n",
       " 'analogous',\n",
       " 'analogously',\n",
       " 'analogue',\n",
       " 'analogy',\n",
       " 'analyse',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'analytic',\n",
       " 'analytical',\n",
       " 'analytically',\n",
       " 'analyze',\n",
       " 'anaphora',\n",
       " 'anation',\n",
       " 'anatomically',\n",
       " 'ancestor',\n",
       " 'anchor',\n",
       " 'ancient',\n",
       " 'and',\n",
       " 'ande',\n",
       " 'anderson',\n",
       " 'andersondarle',\n",
       " 'andor',\n",
       " 'andrej',\n",
       " 'andrew',\n",
       " 'android',\n",
       " 'andronikashvili',\n",
       " 'anecdote',\n",
       " 'angel',\n",
       " 'angeles',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'angry',\n",
       " 'angstrom',\n",
       " 'angular',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'anne',\n",
       " 'annihilate',\n",
       " 'anniversary',\n",
       " 'annotate',\n",
       " 'annotation',\n",
       " 'announce',\n",
       " 'announcement',\n",
       " 'announcer',\n",
       " 'annoy',\n",
       " 'annoying',\n",
       " 'annoyingly',\n",
       " 'annual',\n",
       " 'anomaly',\n",
       " 'anonymous',\n",
       " 'anonymously',\n",
       " 'ans',\n",
       " 'answering',\n",
       " 'ant',\n",
       " 'ante',\n",
       " 'antecedent',\n",
       " 'antenna',\n",
       " 'anthropomorphize',\n",
       " 'anti',\n",
       " 'antibiotic',\n",
       " 'anticipate',\n",
       " 'anticipation',\n",
       " 'antiderivative',\n",
       " 'antidiagonal',\n",
       " 'antidifferentiate',\n",
       " 'antidifferentiation',\n",
       " 'antisymmetric',\n",
       " 'antisymmetrization',\n",
       " 'antisymmetrize',\n",
       " 'antisymmetry',\n",
       " 'anybody',\n",
       " 'anybodys',\n",
       " 'anydensional',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyte',\n",
       " 'anything',\n",
       " 'anyways',\n",
       " 'ap',\n",
       " 'apache',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'aperiodic',\n",
       " 'apex',\n",
       " 'api',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'apostrophe',\n",
       " 'app',\n",
       " 'apparatus',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appearing',\n",
       " 'append',\n",
       " 'appendix',\n",
       " 'appetite',\n",
       " 'appetizer',\n",
       " 'applause',\n",
       " 'apple',\n",
       " 'appliance',\n",
       " 'applicability',\n",
       " 'applicable',\n",
       " 'applicant',\n",
       " 'application',\n",
       " 'applie',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'appoint',\n",
       " 'appointment',\n",
       " 'appreciate',\n",
       " 'appreciation',\n",
       " 'apprentice',\n",
       " 'approach',\n",
       " 'approaches',\n",
       " 'appropriate',\n",
       " 'appropriately',\n",
       " 'approval',\n",
       " 'approve',\n",
       " 'approxa',\n",
       " 'approxate',\n",
       " 'approxately',\n",
       " 'approxation',\n",
       " 'approxator',\n",
       " 'april',\n",
       " 'aq',\n",
       " 'ar',\n",
       " 'arabic',\n",
       " 'arbitrarily',\n",
       " 'arbitrary',\n",
       " 'arborally',\n",
       " 'arc',\n",
       " 'arcane',\n",
       " 'arch',\n",
       " 'archaeology',\n",
       " 'archede',\n",
       " 'architect',\n",
       " 'architecture',\n",
       " 'arcsine',\n",
       " 'arctan',\n",
       " 'arctangent',\n",
       " 'area',\n",
       " 'arg',\n",
       " 'argc',\n",
       " 'argmax',\n",
       " 'argon',\n",
       " 'arguably',\n",
       " 'argue',\n",
       " 'arguement',\n",
       " 'argument',\n",
       " 'argumentation',\n",
       " 'argv',\n",
       " 'ariel',\n",
       " 'arise',\n",
       " 'arithmetic',\n",
       " 'arithmetically',\n",
       " 'arm',\n",
       " 'army',\n",
       " 'arnold',\n",
       " 'arrange',\n",
       " 'arrangement',\n",
       " 'array',\n",
       " 'arrest',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrow',\n",
       " 'arrowhead',\n",
       " 'arsenal',\n",
       " 'art',\n",
       " 'article',\n",
       " 'articulate',\n",
       " 'artifact',\n",
       " 'artificial',\n",
       " 'artificially',\n",
       " 'artist',\n",
       " 'artwork',\n",
       " 'arun',\n",
       " 'arv',\n",
       " 'arxiv',\n",
       " 'ascend',\n",
       " 'ascent',\n",
       " 'ascertain',\n",
       " 'ascii',\n",
       " 'ashamed',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'asking',\n",
       " 'asleep',\n",
       " 'aspect',\n",
       " 'aspire',\n",
       " 'asquare',\n",
       " 'ass',\n",
       " 'assemble',\n",
       " 'assembly',\n",
       " 'assert',\n",
       " 'assertion',\n",
       " 'assess',\n",
       " 'assessment',\n",
       " 'asset',\n",
       " 'asshole',\n",
       " 'assign',\n",
       " 'assigning',\n",
       " 'assignment',\n",
       " 'assilate',\n",
       " 'assistant',\n",
       " 'associa',\n",
       " 'associate',\n",
       " 'associated',\n",
       " 'association',\n",
       " 'associative',\n",
       " 'associativity',\n",
       " 'assumed',\n",
       " 'assumption',\n",
       " 'assurance',\n",
       " 'assure',\n",
       " 'asterisk',\n",
       " 'asteroid',\n",
       " 'astonish',\n",
       " 'astonishingly',\n",
       " 'astronomical',\n",
       " 'astronomy',\n",
       " 'astute',\n",
       " 'asymmetric',\n",
       " 'asymmetry',\n",
       " 'asympto',\n",
       " 'asymptote',\n",
       " 'asymptotic',\n",
       " 'asymptotically',\n",
       " 'asynchronous',\n",
       " 'asynchronously',\n",
       " 'atampt',\n",
       " 'athe',\n",
       " 'atlanta',\n",
       " 'atlantic',\n",
       " 'atm',\n",
       " 'atmosphere',\n",
       " 'atmost',\n",
       " 'atom',\n",
       " 'atomic',\n",
       " 'atop',\n",
       " 'atrocious',\n",
       " 'atrociously',\n",
       " 'att',\n",
       " 'attach',\n",
       " 'attachment',\n",
       " 'attack',\n",
       " 'attacker',\n",
       " 'attain',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attendance',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attorney',\n",
       " 'attract',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'attribu',\n",
       " 'attribute',\n",
       " 'attributed',\n",
       " 'au',\n",
       " 'auction',\n",
       " 'audio',\n",
       " 'audition',\n",
       " 'auditorium',\n",
       " 'augment',\n",
       " 'augmentation',\n",
       " 'augmenting',\n",
       " 'august',\n",
       " 'aus',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'authentic',\n",
       " 'authenticate',\n",
       " 'authentication',\n",
       " 'authenticity',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'auto',\n",
       " 'autocompe',\n",
       " 'automa',\n",
       " 'automata',\n",
       " 'automate',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'automaton',\n",
       " 'automobile',\n",
       " 'autonomous',\n",
       " 'auxiliary',\n",
       " 'av',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'ave',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'averaging',\n",
       " 'averse',\n",
       " 'avert',\n",
       " 'avl',\n",
       " 'avogadro',\n",
       " 'avoid',\n",
       " 'aw',\n",
       " 'await',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awkwardly',\n",
       " 'awkwardness',\n",
       " 'aww',\n",
       " 'ax',\n",
       " 'axb',\n",
       " 'axe',\n",
       " 'axi',\n",
       " 'axiom',\n",
       " 'axiomatic',\n",
       " 'axis',\n",
       " 'axon',\n",
       " 'axp',\n",
       " 'axs',\n",
       " 'axy',\n",
       " 'ay',\n",
       " 'azure',\n",
       " 'ba',\n",
       " 'baby',\n",
       " 'babylonians',\n",
       " 'back',\n",
       " 'backbone',\n",
       " 'backgammon',\n",
       " 'background',\n",
       " 'backlash',\n",
       " 'backpack',\n",
       " 'backprop',\n",
       " 'backpropagate',\n",
       " 'backpropagation',\n",
       " 'backslash',\n",
       " 'backsolve',\n",
       " 'backtrack',\n",
       " 'backtracking',\n",
       " 'backup',\n",
       " 'backward',\n",
       " 'backwards',\n",
       " 'bacteria',\n",
       " 'bacterial',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'badness',\n",
       " 'baffle',\n",
       " 'bag',\n",
       " 'baggage',\n",
       " 'bagofword',\n",
       " 'bail',\n",
       " 'bake',\n",
       " 'balagovic',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'balancer',\n",
       " 'balancing',\n",
       " 'balcony',\n",
       " 'ball',\n",
       " 'balloon',\n",
       " 'ballpark',\n",
       " 'balls',\n",
       " 'baloney',\n",
       " 'bam',\n",
       " 'ban',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'bandwidth',\n",
       " 'bang',\n",
       " 'bank',\n",
       " 'banker',\n",
       " 'banking',\n",
       " 'bankofamericacom',\n",
       " 'bankrupt',\n",
       " 'bar',\n",
       " 'barack',\n",
       " 'barak',\n",
       " 'barbell',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'bark',\n",
       " 'barking',\n",
       " 'barn',\n",
       " 'barnyard',\n",
       " 'barrack',\n",
       " 'barrel',\n",
       " 'barrier',\n",
       " 'barry',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'baseline',\n",
       " 'basement',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'basket',\n",
       " 'basketball',\n",
       " 'bastardization',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'bath',\n",
       " 'bathroom',\n",
       " 'batter',\n",
       " 'battery',\n",
       " 'batting',\n",
       " 'battle',\n",
       " 'baye',\n",
       " 'bayes',\n",
       " 'bayesian',\n",
       " 'bb',\n",
       " 'bbgky',\n",
       " 'bc',\n",
       " 'bd',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beam',\n",
       " 'bean',\n",
       " 'bear',\n",
       " 'bearing',\n",
       " 'beast',\n",
       " 'beat',\n",
       " 'beats',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'bec',\n",
       " 'bed',\n",
       " 'bedrock',\n",
       " 'bedroom',\n",
       " 'bedte',\n",
       " 'bee',\n",
       " 'beep',\n",
       " 'beer',\n",
       " 'beg',\n",
       " 'begin',\n",
       " 'beginner',\n",
       " 'beginning',\n",
       " 'behalf',\n",
       " 'behave',\n",
       " 'behaved',\n",
       " 'behaving',\n",
       " 'behavior',\n",
       " 'behold',\n",
       " 'being',\n",
       " 'belabor',\n",
       " 'belief',\n",
       " 'believable',\n",
       " 'believe',\n",
       " 'believed',\n",
       " 'bell',\n",
       " 'bellman',\n",
       " 'bellmanford',\n",
       " 'bellshape',\n",
       " 'belong',\n",
       " 'belonging',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'bench',\n",
       " 'benchmark',\n",
       " 'bend',\n",
       " 'bender',\n",
       " 'beneath',\n",
       " 'beneficial',\n",
       " 'beneficiary',\n",
       " 'benefit',\n",
       " 'bengio',\n",
       " 'bengios',\n",
       " 'benign',\n",
       " 'bent',\n",
       " 'bentley',\n",
       " 'ber',\n",
       " 'berkeley',\n",
       " 'berkman',\n",
       " 'bernoulli',\n",
       " 'bernoullis',\n",
       " 'berwick',\n",
       " 'best',\n",
       " 'bestcase',\n",
       " 'bestfirst',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'beth',\n",
       " 'betray',\n",
       " 'better',\n",
       " 'beware',\n",
       " 'bfs',\n",
       " 'bg',\n",
       " 'bi',\n",
       " 'bias',\n",
       " 'biased',\n",
       " 'biasvariance',\n",
       " 'bible',\n",
       " 'bicycle',\n",
       " 'bid',\n",
       " 'bidding',\n",
       " 'bidirectional',\n",
       " 'bieber',\n",
       " 'bigger',\n",
       " 'biggie',\n",
       " 'bigoh',\n",
       " 'bigram',\n",
       " 'bijection',\n",
       " 'bijective',\n",
       " 'bike',\n",
       " 'bilinear',\n",
       " 'bilingual',\n",
       " 'bill',\n",
       " 'billiard',\n",
       " 'billion',\n",
       " 'billionaire',\n",
       " 'bin',\n",
       " 'binary',\n",
       " 'bind',\n",
       " 'binding',\n",
       " 'bing',\n",
       " 'bingo',\n",
       " 'binomial',\n",
       " 'bio',\n",
       " 'biological',\n",
       " 'biologist',\n",
       " 'biology',\n",
       " 'bipartite',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'birthdeath',\n",
       " 'bis',\n",
       " 'bisection',\n",
       " 'bisector',\n",
       " 'biswa',\n",
       " 'biswas',\n",
       " 'bite',\n",
       " 'bitmap',\n",
       " 'bitonic',\n",
       " 'bits',\n",
       " 'bitter',\n",
       " 'bitty',\n",
       " 'bitwise',\n",
       " 'bivariate',\n",
       " 'bizarre',\n",
       " 'bizarro',\n",
       " 'bj',\n",
       " 'bk',\n",
       " 'bl',\n",
       " 'black',\n",
       " 'blackboard',\n",
       " 'blackjack',\n",
       " 'blackschole',\n",
       " 'blackwell',\n",
       " 'blade',\n",
       " 'blah',\n",
       " 'blame',\n",
       " 'blank',\n",
       " 'blanket',\n",
       " 'blast',\n",
       " 'blatant',\n",
       " 'blatantly',\n",
       " 'blend',\n",
       " 'bless',\n",
       " 'blessing',\n",
       " 'bleu',\n",
       " 'blind',\n",
       " 'blindly',\n",
       " 'blink',\n",
       " 'blip',\n",
       " 'blitzstein',\n",
       " 'blob',\n",
       " 'blobs',\n",
       " 'block',\n",
       " 'blockbuster',\n",
       " 'blocking',\n",
       " 'blockwise',\n",
       " 'blog',\n",
       " 'blood',\n",
       " 'bloodstream',\n",
       " 'bloody',\n",
       " 'bloom',\n",
       " 'blossom',\n",
       " 'blow',\n",
       " 'blowup',\n",
       " 'blue',\n",
       " 'blunder',\n",
       " 'blunt',\n",
       " 'blur',\n",
       " 'blurb',\n",
       " 'blurry',\n",
       " 'bm',\n",
       " 'bmw',\n",
       " 'bn',\n",
       " 'bns',\n",
       " 'bo',\n",
       " 'board',\n",
       " 'boas',\n",
       " 'boat',\n",
       " 'bob',\n",
       " 'bobby',\n",
       " 'bobo',\n",
       " 'bobs',\n",
       " 'bode',\n",
       " 'body',\n",
       " 'bogge',\n",
       " 'bogus',\n",
       " 'boil',\n",
       " 'boilerplate',\n",
       " 'boiling',\n",
       " 'bokeepe',\n",
       " 'bold',\n",
       " 'bolde',\n",
       " 'boldface',\n",
       " 'bolt',\n",
       " 'boltzmann',\n",
       " 'bomb',\n",
       " 'bond',\n",
       " 'bone',\n",
       " 'bonferroni',\n",
       " 'bonus',\n",
       " 'boo',\n",
       " 'bool',\n",
       " 'boolean',\n",
       " 'booleans',\n",
       " 'boom',\n",
       " 'boost',\n",
       " 'boosting',\n",
       " 'boot',\n",
       " 'booth',\n",
       " 'bootstrap',\n",
       " 'border',\n",
       " 'borderline',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'borrow',\n",
       " 'borrowing',\n",
       " 'bos',\n",
       " 'bose',\n",
       " 'boseeinstein',\n",
       " 'boson',\n",
       " 'bosonic',\n",
       " 'boss',\n",
       " 'boston',\n",
       " 'bostore',\n",
       " 'bot',\n",
       " 'bother',\n",
       " 'bothersome',\n",
       " 'bottle',\n",
       " 'bottleneck',\n",
       " 'bottom',\n",
       " 'bottommost',\n",
       " 'bottomup',\n",
       " 'bounce',\n",
       " 'bound',\n",
       " 'boundary',\n",
       " 'bounded',\n",
       " 'bow',\n",
       " 'bowden',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'bp',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3: Reduce Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:14:50.289480Z",
     "start_time": "2019-11-13T22:14:50.284245Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_dim(sm):\n",
    "    # define Truncated SVD\n",
    "    lsa = TruncatedSVD(40)\n",
    "    # do LSA on sparse matrix \"sm\"\n",
    "    rd = normalize(lsa.fit_transform(sm))\n",
    "    \n",
    "    # print explained variance ratio\n",
    "#     print(\"LSA Explained Variance Ratio: \",lsa.explained_variance_ratio_)\n",
    "    # return matrix of reduced dimensions\n",
    "    return rd, lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:14:53.706457Z",
     "start_time": "2019-11-13T22:14:51.965830Z"
    }
   },
   "outputs": [],
   "source": [
    "rd_v, lsa_components = reduce_dim(sm_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:14:54.514657Z",
     "start_time": "2019-11-13T22:14:54.507431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29060678,  0.20256223,  0.11349155, ...,  0.13670003,\n",
       "        -0.05455222,  0.10012089],\n",
       "       [ 0.18613807,  0.24687721, -0.05022695, ...,  0.03007343,\n",
       "         0.22704304, -0.04304533],\n",
       "       [ 0.18630519,  0.14359342,  0.22163487, ...,  0.02878994,\n",
       "        -0.02413465, -0.10814504],\n",
       "       ...,\n",
       "       [ 0.62592726, -0.01189198,  0.08850105, ..., -0.04721063,\n",
       "         0.05601461, -0.04634379],\n",
       "       [ 0.16501553,  0.14118581,  0.34023781, ...,  0.03010239,\n",
       "         0.11953425,  0.0440007 ],\n",
       "       [ 0.35590365,  0.2694001 ,  0.15576531, ..., -0.12991931,\n",
       "         0.13463747, -0.01787092]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd_v\n",
    "\n",
    "# identify which rows come from each subject\n",
    "# get the slice of rd_v that correspond to a subject\n",
    "# plot a heat map of those rows\n",
    "# --> see which topics are strongly associated with that subject\n",
    "# compare subjects/compute entropy per subject (i.e. core math discipline vs. interdisciplinary field e.g. NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:15:05.021479Z",
     "start_time": "2019-11-13T22:15:05.013304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.39615647e-04,  1.45045760e-04,  7.06957133e-04, ...,\n",
       "         4.33000734e-04,  2.02920909e-03,  1.81086131e-04],\n",
       "       [-1.91425611e-05,  2.32506343e-04, -3.68802959e-04, ...,\n",
       "         2.96096316e-04, -1.28958949e-03,  2.30159599e-04],\n",
       "       [ 5.63842074e-04, -1.70416514e-04, -9.42271902e-06, ...,\n",
       "         7.22792970e-05, -4.91809037e-05,  1.99960048e-05],\n",
       "       ...,\n",
       "       [ 7.70298825e-04,  4.92233755e-04, -1.90660231e-03, ...,\n",
       "         1.12619626e-03,  6.55838764e-03,  2.00357470e-04],\n",
       "       [ 1.88365178e-03,  3.54845764e-04,  2.67391462e-04, ...,\n",
       "        -1.39857193e-03, -8.74487026e-05, -1.42066495e-04],\n",
       "       [-9.12787189e-04, -4.19588869e-04, -1.02706319e-04, ...,\n",
       "        -8.03684606e-04, -6.39529421e-04, -1.20599454e-04]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:15:09.840935Z",
     "start_time": "2019-11-13T22:15:09.832309Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model_components, feature_names, num_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model_components):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:15:11.939799Z",
     "start_time": "2019-11-13T22:15:11.852944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "vector, matrix, column, row, probability, equation, solution, transpose, transform, plane\n",
      "\n",
      "Topic  1\n",
      "probability, event, random, conditional, node, distribution, outcome, log, sample, tree\n",
      "\n",
      "Topic  2\n",
      "node, tree, list, log, edge, search, insert, algorithm, subtree, link\n",
      "\n",
      "Topic  3\n",
      "laplace_transform, equation, integral, derivative, solution, differential, pre, delta, sine, theta\n",
      "\n",
      "Topic  4\n",
      "vector, dot, plane, length, transform, scalar, direction, span, member, component\n",
      "\n",
      "Topic  5\n",
      "laplace_transform, node, st, probability, event, infinity, integral, vector, sine, row\n",
      "\n",
      "Topic  6\n",
      "solution, equation, differential, event, probability, derivative, slope, node, initial, pre\n",
      "\n",
      "Topic  7\n",
      "node, lambda, tree, subtree, integral, link, address, child, pdf, variance\n",
      "\n",
      "Topic  8\n",
      "edge, vertex, path, graph, weight, cycle, short, algorithm, delta, tree\n",
      "\n",
      "Topic  9\n",
      "row, integral, determinant, theta, dx, area, curve, field, plane, delta\n",
      "\n",
      "Topic  10\n",
      "eigenvalue, eigenvector, matrix, lambda, event, integral, delta, determinant, field, probability\n",
      "\n",
      "Topic  11\n",
      "log, theta, particle, event, tree, hash, beta, temperature, energy, search\n",
      "\n",
      "Topic  12\n",
      "state, particle, sub, arrival, markov, temperature, chain, energy, renewal, row\n",
      "\n",
      "Topic  13\n",
      "transpose, node, model, theta, neural, datum, beta, error, hypothesis, network\n",
      "\n",
      "Topic  14\n",
      "particle, list, temperature, energy, transpose, null, beta, edge, gas, column\n",
      "\n",
      "Topic  15\n",
      "determinant, theta, mu, particle, beta, submatrix, transform, temperature, node, dot\n",
      "\n",
      "Topic  16\n",
      "theta, sub, state, markov, arrival, chain, renewal, list, cosine, transpose\n",
      "\n",
      "Topic  17\n",
      "transform, theta, composition, matrix, member, apply, rn, mapping, codomain, identity\n",
      "\n",
      "Topic  18\n",
      "psi, transpose, partial, derivative, matrix, transform, hash, differential, sub, expectation\n",
      "\n",
      "Topic  19\n",
      "tree, root, transpose, dot, string, subtree, height, length, opencourseware, ana\n",
      "\n",
      "Topic  20\n",
      "psi, partial, string, tree, lambda, ana, dx, code, program, integral\n",
      "\n",
      "Topic  21\n",
      "plane, transform, solution, field, normal, equation, matrix, surface, laplace_transform, curl\n",
      "\n",
      "Topic  22\n",
      "hash, expectation, conditional, cosine, state, model, sine, length, table, pi\n",
      "\n",
      "Topic  23\n",
      "delta, hash, partial, plane, psi, pivot, stability, method, lambda, eigenvalue\n",
      "\n",
      "Topic  24\n",
      "hash, alpha, hypothesis, slot, table, error, key, pre, lambda, collision\n",
      "\n",
      "Topic  25\n",
      "outcome, psi, expectation, random, partial, variance, experent, roll, coin, expect\n",
      "\n",
      "Topic  26\n",
      "plane, pdf, psi, sine, cosine, cdf, normal, alpha, hash, sequence\n",
      "\n",
      "Topic  27\n",
      "car, outcome, object, element, pdf, opencourseware, ana, class, color, hash\n",
      "\n",
      "Topic  28\n",
      "slope, theta, sequence, outcome, subset, subspace, population, element, lambda, differential\n",
      "\n",
      "Topic  29\n",
      "population, random, log, psi, differential, side, hypothesis, probability, student, temperature\n",
      "\n",
      "Topic  30\n",
      "plane, population, log, mu, series, pi, row, sine, transform, head\n",
      "\n",
      "Topic  31\n",
      "sequence, delta, alpha, converge, log, block, outcome, row, series, convergence\n",
      "\n",
      "Topic  32\n",
      "subspace, determinant, column, null, combination, span, submatrix, population, basis, integral\n",
      "\n",
      "Topic  33\n",
      "population, subproblem, card, toss, path, algorithm, coin, node, head, length\n",
      "\n",
      "Topic  34\n",
      "population, mu, hash, delta, transpose, element, variance, cosine, sine, hypothesis\n",
      "\n",
      "Topic  35\n",
      "alpha, list, expectation, subspace, variance, student, object, psi, population, infinity\n",
      "\n",
      "Topic  36\n",
      "column, mu, lambda, arrival, transform, transpose, list, datum, equation, object\n",
      "\n",
      "Topic  37\n",
      "population, alpha, list, column, toss, search, slope, transpose, coin, sine\n",
      "\n",
      "Topic  38\n",
      "outcome, stack, alpha, array, state, conditional, roll, pdf, datum, event\n",
      "\n",
      "Topic  39\n",
      "coin, toss, mu, head, matrix, costan, distribution, pre, error, probability\n"
     ]
    }
   ],
   "source": [
    "# x = df_s['text']\n",
    "# cv_tfidf = TfidfVectorizer(stop_words='english')\n",
    "# sm = cv_tfidf.fit_transform(x)\n",
    "# lsa = TruncatedSVD(10)\n",
    "# rd = lsa.fit_transform(sm)\n",
    "display_topics(lsa_components, feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:15:25.844729Z",
     "start_time": "2019-11-13T22:15:25.837586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Linear Algebra', 'Probability', 'CS', 'Diff. Eq.', 'Algorithms',\n",
       "       'Statistics', 'Calculus', 'Data Structures', 'AI', 'Math for Eng.',\n",
       "       'NLP'],\n",
       "      dtype='object', name='label')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_sorted.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:15:35.796142Z",
     "start_time": "2019-11-13T22:15:35.565430Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pickle_open = open('df.pickle', 'wb')\n",
    "pickle.dump(df, df_pickle_open)\n",
    "df_pickle_open.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:15:36.333797Z",
     "start_time": "2019-11-13T22:15:36.291164Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pickle_read = open('df.pickle', 'rb')\n",
    "df_pickle = pickle.load(df_pickle_read)\n",
    "df_pickle_read.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:15:37.046825Z",
     "start_time": "2019-11-13T22:15:37.018617Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>sequence segment review mathematical backgroun...</td>\n",
       "      <td>Probability</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>CS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>Math for Eng.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>music   david j malan     end week   see cir...</td>\n",
       "      <td>Diff. Eq.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>857</td>\n",
       "      <td>follow content provide   opencourseware creati...</td>\n",
       "      <td>Linear Algebra</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>AI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>Diff. Eq.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>860 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text           label  le\n",
       "0    follow content provide creative common license...        Calculus   3\n",
       "1    sequence segment review mathematical backgroun...     Probability   9\n",
       "2    follow content provide creative common license...              CS   2\n",
       "3    follow content provide creative common license...      Algorithms   1\n",
       "4    follow content provide creative common license...      Algorithms   1\n",
       "..                                                 ...             ...  ..\n",
       "855  follow content provide creative common license...   Math for Eng.   7\n",
       "856    music   david j malan     end week   see cir...       Diff. Eq.   5\n",
       "857  follow content provide   opencourseware creati...  Linear Algebra   6\n",
       "858  follow content provide creative common license...              AI   0\n",
       "859  follow content provide creative common license...       Diff. Eq.   5\n",
       "\n",
       "[860 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:15:39.541648Z",
     "start_time": "2019-11-13T22:15:39.523018Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>sequence segment review mathematical backgroun...</td>\n",
       "      <td>Probability</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>CS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>Algorithms</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>Math for Eng.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>music   david j malan     end week   see cir...</td>\n",
       "      <td>Diff. Eq.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>857</td>\n",
       "      <td>follow content provide   opencourseware creati...</td>\n",
       "      <td>Linear Algebra</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>AI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859</td>\n",
       "      <td>follow content provide creative common license...</td>\n",
       "      <td>Diff. Eq.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>860 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text           label  le\n",
       "0    follow content provide creative common license...        Calculus   3\n",
       "1    sequence segment review mathematical backgroun...     Probability   9\n",
       "2    follow content provide creative common license...              CS   2\n",
       "3    follow content provide creative common license...      Algorithms   1\n",
       "4    follow content provide creative common license...      Algorithms   1\n",
       "..                                                 ...             ...  ..\n",
       "855  follow content provide creative common license...   Math for Eng.   7\n",
       "856    music   david j malan     end week   see cir...       Diff. Eq.   5\n",
       "857  follow content provide   opencourseware creati...  Linear Algebra   6\n",
       "858  follow content provide creative common license...              AI   0\n",
       "859  follow content provide creative common license...       Diff. Eq.   5\n",
       "\n",
       "[860 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T23:11:17.924349Z",
     "start_time": "2019-11-13T23:11:17.528826Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('grouped_sorted.pickle', 'wb') as writefile:\n",
    "    pickle.dump(grouped_sorted, writefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T23:12:23.262696Z",
     "start_time": "2019-11-13T23:12:23.130170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>le</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Linear Algebra</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Probability</td>\n",
       "      <td>124</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CS</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Diff. Eq.</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Algorithms</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Statistics</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Calculus</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Data Structures</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AI</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Math for Eng.</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NLP</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text   le\n",
       "label                     \n",
       "Linear Algebra    152  152\n",
       "Probability       124  124\n",
       "CS                104  104\n",
       "Diff. Eq.          93   93\n",
       "Algorithms         81   81\n",
       "Statistics         79   79\n",
       "Calculus           70   70\n",
       "Data Structures    62   62\n",
       "AI                 48   48\n",
       "Math for Eng.      28   28\n",
       "NLP                19   19"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('grouped_sorted.pickle', 'rb') as readfile:\n",
    "    gs = pickle.load(readfile)\n",
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Function 4: Cluster/Visualize (irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T00:09:05.715087Z",
     "start_time": "2019-11-13T00:09:05.701552Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# From KMeansClustering.ipynb: \n",
    "# helper function that allows us to display data in 2 dimensions and highlights the clusters\n",
    "def display_cluster(X,km=[],num_clusters=0):\n",
    "    color = 'brgcmyk'\n",
    "    alpha = 0.5\n",
    "    s = 20\n",
    "    if num_clusters == 0:\n",
    "        plt.scatter(X[:,0],X[:,1],c = color[0],alpha = alpha,s = s)\n",
    "    else:\n",
    "        for i in range(num_clusters):\n",
    "            plt.scatter(X[km.labels_==i,0],X[km.labels_==i,1],c = color[i],alpha = alpha,s=s)\n",
    "            plt.scatter(km.cluster_centers_[i][0],km.cluster_centers_[i][1],c = color[i], marker = 'x', s = 100)\n",
    "            \n",
    "def cluster(rd, num_clusters = 5):\n",
    "    km = KMeans(n_clusters=num_clusters,random_state=10,n_init=10) # n_init, number of times the K-mean algorithm will run\n",
    "    km.fit(rd)\n",
    "    display_cluster(rd,km,num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T00:29:52.692366Z",
     "start_time": "2019-11-13T00:29:52.256477Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cluster(rd_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T00:13:27.447899Z",
     "start_time": "2019-11-13T00:13:27.439118Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T00:13:29.602338Z",
     "start_time": "2019-11-13T00:13:29.014090Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X = rd_v\n",
    "sns.scatterplot(X[:,0],X[:,1],hue = df.label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Try UMAP (irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T00:28:54.450512Z",
     "start_time": "2019-11-13T00:28:51.492841Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42, n_neighbors = 50)\n",
    "reducer.fit(rd_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T00:28:54.728840Z",
     "start_time": "2019-11-13T00:28:54.713971Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embedding = reducer.transform(rd_v)\n",
    "# Verify that the result of calling transform is\n",
    "# idenitical to accessing the embedding_ attribute\n",
    "assert(np.all(embedding == reducer.embedding_))\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T00:28:55.932882Z",
     "start_time": "2019-11-13T00:28:55.106288Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# palette = [sns.color_palette()[x] for x in df['le']]\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=df['le'], cmap='Spectral', s=8)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('UMAP projection of Math Subjects', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Testing some ideas (irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:29:51.376809Z",
     "start_time": "2019-11-11T23:29:51.365094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import these modules \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "ps = PorterStemmer() \n",
    "\n",
    "# choose some words to be stemmed \n",
    "words = [\"matrix\", \"matrices\", \"transform\", \"transformation\", \"probability\", \"probabilities\"] \n",
    "\n",
    "for w in words: \n",
    "\tprint(w, \" : \", ps.stem(w)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:29:58.001873Z",
     "start_time": "2019-11-11T23:29:57.991182Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "words = [\"matrix\", \"matrices\", \"transform\", \"transformation\", \"probability\", \"probabilities\"] \n",
    "\n",
    "for w in words: \n",
    "\tprint(w, \" : \", stemmer.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:30:36.402800Z",
     "start_time": "2019-11-11T23:30:36.387539Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "sb = SnowballStemmer(\"english\")\n",
    "words = [\"matrix\", \"matrices\", \"transform\", \"transformation\", \"probability\", \"probabilities\"] \n",
    "\n",
    "for w in words: \n",
    "\tprint(w, \" : \", sb.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:46:14.107707Z",
     "start_time": "2019-11-11T23:46:08.252912Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "words = [\"matrix\", \"matrices\", \"transform\", \"transformation\", \"probability\", \"probabilities\"] \n",
    "\n",
    "for w in words: \n",
    "\tprint(w, \" : \", wn.lemmatize(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T21:48:33.748098Z",
     "start_time": "2019-11-12T21:48:33.732927Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "words = [\"matrix\", \"matrices\", \"transform\", \"transformation\", \"probability\", \"probabilities\",\"laplace_transformation\"] \n",
    "\n",
    "for w in words: \n",
    "\tprint(w, \" : \", wn.lemmatize(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T21:50:25.805888Z",
     "start_time": "2019-11-12T21:50:11.195216Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "tokenizer = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T21:51:31.735892Z",
     "start_time": "2019-11-12T21:51:31.718433Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenize_lemma(text):\n",
    "        \n",
    "    text_obj = tokenizer(text, disable=['parser', 'ner'])\n",
    "    \n",
    "    text_lemma = ' '.join([token.lemma_ for token in text_obj if not token.is_stop])\n",
    "  \n",
    "    return text_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T21:52:31.159467Z",
     "start_time": "2019-11-12T21:52:31.132717Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_text = \"In mathematics, the laplace_transform is an integral transform named after its inventor Pierre-Simon Laplace. It is also known as a laplace_transformation. \"\n",
    "tokenize_lemma(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
